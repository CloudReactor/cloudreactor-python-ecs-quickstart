# How to set up a simple ETL (or other!) task using ECS and CloudReactor

## Why should I use ECS?

Using AWS ECS with the Fargate execution method to run tasks has several benefits:

1) Near infinite scalability -- run as many task instances as you want at any time
2) No need to manage EC2 instances -- security patches, library upgrades, library
compatibility issues, downtime. This leads to increased reliability and more time for 
developers.
3) Running Docker images leads to more predictable, isolated execution. Here's [a good summary
of the advantages of Docker](https://www.linode.com/docs/applications/containers/when-and-why-to-use-docker/).
4) Only pay for the CPU/memory you reserve while your tasks are running
5) Reliable scheduling by AWS without a separate scheduling server

## Why should I use CloudReactor?

The volume and complexity of data generated by companies is always growing: your Customer Success team just set up a ticket-tracking system and wants to pull metrics from it. Engineering wants to monitor data deliveries to customers, or from suppliers. Finance wants to join marketing data with revenue data to measure lifetime value by channel.

All these requests require data ETL tasks to be written, and over time, these tasks accumulate. As they accumulate, ensuring these tasks run smoothly takes up more and more engineering time. For example, understanding the current status of each of these tasks becomes more burdensome. Because there are so many different tasks, it becomes harder to know when a given task fails. If a task fails or encounters an error, you need to quickly figure out what the error was, and remediate -- perhaps restarting the infrastructure and task, perhaps pinpointing and fixing the code or data issue.

**CloudReactor allows you to easily monitor and manage all these different tasks.** In doing so, we want to reduce the time and effort your organization spends on DevOps -- time and effort that could be spent on higher priority areas -- and improve the robustness, reliability, and transparency of your infrastructure.

With CloudReactor, you can easily:
1) View the status of all tasks on a single page
2) Change a task's execution schedule with just a couple of clicks
3) Receive notifications of failures
4) Set automatic retries -- no more manually ssh-ing into a box to kick off a failed task
5) Manually re-run tasks at the click of a button
6) Enforce concurrency limits (e.g. only allow one instance of a task to run at the same time)
7) Track which code ran in a given task; commit links allow you to easily jump between a task and the underlying code
8) Record and view statistics about a given task -- how long it ran for, how many rows of data was tasked, etc.
9) Create and execute workflows consisting of tasks, which transition based on task exit code
or other conditions

## Makes sense. So how do I get started?

### Setup ECS

If you haven't yet setup ECS in your AWS account, follow the steps below.

Note that setting up ECS (as below) is entirely separate from setting up CloudReactor.

1) Go to https://aws.amazon.com/ecs/getting-started/
2) Click the `ECS console walkthrough` button
3) Log in to AWS if necessary
4) Change the region to your default AWS region
5) Click the `Get started` button
6) Choose the `nginx` container image and click the `Next` button
7) On the next page, the defaults are sufficient, so hit `Next` again
8) On the next page, name your cluster the desired name of your deployment environment -- for example `staging`. If you have an existing VPC and subnets you want to use to run your tasks, you can select them here. Otherwise, the console will create a new VPC and subnets for you.
After entering your desired cluster name, hit `Next` again.
9) On the next and final page, review your settings and hit the `Create` button. You'll see the status of the created resources on the next page. **If you didn't choose existing subnets, record the subnet IDs -- we'll use them for the deployment of this project.**

After these steps, AWS should create:

1) A cluster named as you chose on step 8 above.
2) A VPC named `ECS [cluster name] - VPC`
3) 2 subnets in the VPC named `ECS [cluster name] - Public Subnet 1` and `ECS [cluster name] - Public Subnet 2`.
You can see these in VPC .. Subnets. Note that these subnets are public; if you 
want to use private subnets, you'll have to create your own. 
**Record the Subnet IDs -- we'll add them to the Run Environment in CloudReactor.**
4) A security group named `ECS staging - ECS Security Group` in the VPC.
You can find it in `VPC .. Security Groups`. 
**Record the Security Group ID, we'll add it to the Run Environment in CloudReactor.**
5) Once you've recorded the Subnet IDs and Security Group IDs, under "ECS resource creation", you'll see `Cluster [the name of the cluster you created]`. Clicking this link will take you to the cluster's details page; **record the `Cluster ARN`** you see here.

At this point, you have a working ECS environment. 

### Give CloudReactor permissions

To have CloudReactor manage your tasks in your AWS environment, first you need
to give CloudReactor permissions in AWS to run tasks, schedule tasks,
create services, and trigger Workflows by deploying the
[CloudReactor AWS CloudFormation template](https://github.com/CloudReactor/aws-role-template).
Follow the instructions in the [README.md](https://github.com/CloudReactor/aws-role-template/blob/master/README.md), 
being sure to record the ```ExternalID```, ```CloudreactorRoleARN```, ```TaskExecutionRoleARN```,
```WorkflowStarterARN```, and ```WorkflowStarterAccessKey``` values.

### Set up a CloudReactor account

Contact us at support@cloudreactor.io and we'll create an account for you
and give you an API key.
Then login to [processes.cloudreactor.io](https://processes.cloudreactor.io/). 
Now you'll create a Run Environment -- these settings tell CloudReactor how to run tasks in AWS.

1. Click on "Run Environments", then "Add Environment"
2. Name your environment (e.g. "staging", "production"). You may want to keep the
name in all lowercase letters without spaces or symbols besides "-" and "_", so
that filenames and command-lines you'll use later will be sane. 
**Note the exact name of your Run Environment**, as you'll need this later.
3. Fill in your AWS account ID and default region. Your AWS account ID is a 12-digit number that you can find by clicking "Support" then "Support Center". For default region, select the region that you want CloudReactor to run tasks / workflows in (e.g.`us-west-2`).
4. For `Assumable Role ARN`
fill in the value of `CloudreactorRoleARN` from the output of the CloudFormation stack.
5. For `External ID`, use the same External ID you entered when you created the CloudFormation stack.
6. For `Workflow Starter Lambda ARN`, fill in the value of `WorkflowStarterARN` from the output of the CloudFormation stack.
7. For `Workflow Starter Access Key`, fill in the value of `WorkflowStarterAccessKey` from the output of the CloudFormation stack.
8. Add the subnets and security group created by the ECS getting started wizard above
9. Under AWS ECS Settings, choose a `Default Launch Type` of `Fargate` and check FARGATE under Supported Launch Types.
10. For `Default Cluster ARN`, fill in the `Cluster ARN` of the ECS cluster you created above
11. For `Default Execution Role` and `Default Task Role`, fill in the value of
`TaskExecutionRoleARN` from the output of the CloudFormation stack.
12. Click on the `Save` button

Optionally, if you want to be alerted if task executions fail,
you'll need to set up an alerts profile. An alert method profile contains notification settings. To do this:
1. Click on "alert settings", then "create new alert profile".
2. Name the alert profile (e.g. "default"), and fill in your PagerDuty API key
3. Click on the `Save` button

### Get this project's source code

You'll need to get this project's source code onto a filesystem where you can make changes.
You can either clone this project directly, or fork it first, then clone it.

If cloning directly,

    git clone https://github.com/CloudReactor/cloudreactor-ecs-quickstart.git

### Deploy the tasks to AWS and CloudReactor

These steps show how you can deploy the example project in this repo to ECS Fargate
and have its tasks managed by CloudReactor. There are two methods of doing so,
Docker Deployment and Native Deployment. 

#### Docker Deployment

This deployment method is appropriate for when 

* You don't have python installed directly on your machine; or
* you don't want add another set of dependencies to your libraries; or 
* you need to deploy from a Windows machine.

The steps for Docker Deployment are:

1) Ensure you have Docker running locally, and have installed
[Docker Compose](https://docs.docker.com/compose/install/).
   
2) Copy `docker_deploy.env.example` to `docker_deploy.env` and
and fill in your AWS access key, access key secret, and default
region. You may also populate this file with a script you write yourself,
for example with something that uses the AWS CLI to assume a role and gets
temporary credentials.
3) Copy `deploy/vars/example.yml` to `deploy/vars/{environment}.yml`, where `{environment}` is the name
of the Run Environment created above (e.g. `staging`, `production`)
4) Open the .yml file you just created, and enter your CloudReactor API key next to "api_key"
5) Build the Docker container that will deploy the project. In a bash shell, run:


    ```./docker_build_deployer.sh <environment>```     
    
   In a Windows command prompt, run:
   
    ```docker_build_deployer.bat <environment>```
    
`<environment>` is a required argument, which is the name of the Run Environment.     
    
This step is only necessary once, unless you add additional configuration
to ```Dockerfile-deploy```.

6) To deploy, in a bash shell, run:

    ```./docker_deploy.sh <environment> [task_name]```     
    
   In a Windows command prompt, run:
   
    ```docker_deploy.bat <environment>  [task_names]```
    
In both of these commands, `<environment>` is a required argument, which is the
name of the Run Environment. `[task_names]` is an optional argument, which is a
comma-separated list of tasks to be deployed. In this project, this can be one or more of "main", "fail",
"ondemand", etc, separated by commas. If `[task_names]` is omitted, all tasks will be deployed.
  

#### Native Deployment

This deployment method is appropriate for when
 
* You want to deploy from a Linux or Mac OS X machine (virtual machines included); and, 
* you have python installed on the machine (possibly in a virtual environment); and,
* you want to use python running directly on the machine to deploy the project

It has the advantage that you can use the AWS configuration you 
already have set up on that machine for the AWS CLI.
 
This method most likely will not work on Windows machines, though it has
not been tested.

The steps for Native Deployment are: 

1) Ensure you have Docker running locally
2) If desired, create and use a virtual environment for deployment dependencies.
3) Run

   ```pip install -r deploy_requirements.txt```
4) Copy `deploy/vars/example.yml` to `deploy/vars/<environment>.yml`, where `<environment>` is the name
of the Run Environment created above (e.g. `staging`, `production`)
5) Modify `deploy/vars/<environment>.yml` to contain your CloudReactor API key
6) To deploy,

   ```./deploy.sh <environment> [task_names]```


where <environment> is a required argument, which is the
name of the Run Environment. `[task_names]` is an optional argument, which is a
comma-separated list of tasks to be deployed. In this project, this can be one or more of 
"main", "file_io", etc, separated by commas. If `[task_names]` is omitted, all tasks will be deployed.

#### Deployed tasks

Successfully deploying this example project will create a few ECS tasks that
run the code in `main.py` -- a toy task that simply prints a statement and the
numbers 1-30 in a loop. The tasks are listed in `deploy/common.yml`, and have 
the following behavior:

* *main* prints 30 numbers and exits successfully. It is scheduled to run four times per day.
* *fail* fails after 3 loop iterations and exits with an error code. It is scheduled to run twice a day.
* *timeout* prints 300 numbers and would timeout if managed by CloudReactor. It is scheduled to run twice a day.
* *ondemand* also prints 30 numbers and exits successfully. It is not scheduled.
* *with_updates* also prints 30 numbers and exits successfully. While it does so,
it updates the successful count and the last status message that is shown in
CloudReactor, using the status updater library. It is scheduled to run daily.
* *service* loops continuously and never exits
* *file_io* uses [non-persistent file storage](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/fargate-task-storage.html) to write and read numbers
* *web_server* uses a python library dependency (Flask) to implement a web server and shows how to link an 
AWS Application Load Balancer (ALB) to a service. It requires that an ALB and target group
be setup already, so it is not enabled by default. 

### Secret management

In steps 5 and 7 above you most likely added sensitive data to those files,
which you don't want to commit in plaintext. The default configuration assumes
you just keep those secret files locally and don't commit them, thus they are
excluded in .gitignore.

#### Ansible Vault

However, for sharing with a team or to have the secrets in source control for
backup/history reasons, it's better to check the secret files in, but encrypted.
One option for doing that is to use [ansible-vault](https://docs.ansible.com/ansible/latest/user_guide/vault.html)
which is well integrated with ansible.

To use ansible-vault to encrypt secret files, change your directory to /deploy/vars
and run:

    ansible-vault encrypt [environment].yml

ansible-vault will prompt for a password, then encrypt the file. To edit it:

    ansible-vault edit [environment].yml

You should do the same for `deploy/files/.env.[environment]`

Next, change the deployment script deploy.sh to get the encryption password,
either from user input, or an external file or script. Detailed instructions
are in `deploy.sh`.

Finally, uncomment the lines in `.gitignore` that ignore secret files, since
you'll be checking them in encrypted.

#### git-crypt

Another option for encryption is [git-crypt](https://github.com/AGWA/git-crypt), 
which encrypt secrets when they are committed to Git. However, this leaves 
secrets unencrypted in the filesystem where the repository is checked out.

If you set up git-crypt, uncomment the lines in `.gitignore` that ignore 
secret files, since they will be encrypted in the repository.

#### Runtime secrets

As a best practice, secrets needed at runtime should not be present in the Docker
images. Instead you can use AWS Secrets Manager to set environment variables
that the tasks can read during runtime. To do that, add a `secrets` section
to the `extra_task_definition_properties`. See `deploy/common.yml` for an
example on how to do that.

## Deploying your own tasks

Now that you have deployed the example tasks, you can move your existing code to this
project. You can add or modify tasks in `deploy/common.yml` to call the commands you want,
with configuration for the schedule, retry parameters, and environment variables.
Feel free to delete the tasks that you don't need, just by removing the top level keys
in `task_name_to_config`.

### Additional configuration

The project allows for many configuration options to be set or overridden: 

* You can override the common task configuration in each task's configuration in 
the `task_name_to_config` property of `deploy/common.yml`
* Your deployment environment can override the AWS configuration,
the CloudReactor API key, and per task configuration in `deploy/<environment>.yml`.
See `deploy/example.yml` for instructions.
* You can add additional properties to the main container running each task, 
such as `mountPoints` and `portMappings`  by setting   
`extra_task_definition_properties` in common.yml or `deploy/<environment>.yml`.
See the `file_io` task for an example of this.
* You can add AWS ECS task properties, such as `volumes` and `secrets`, 
by setting `extra_main_container_properties` in the `ecs` property of each task
configuration. See the `file_io` task for an example of this.
`extra_task_definition_properties` in common.yml or `deploy/<environment>.yml`.
See the `file_io` task for an example of this.
* You can add additional containers to the task by setting `extra_container_definitions`
in `deploy/common.yml` or `deploy/<environment>.yml`.
* To deploy non-python projects, change `Dockerfile-deploy` to have the dependencies
needed to build your project (JDK, C++ compiler, etc.). Then, if necessary, 
add a build step to `deploy/deploy.yml` (search for "maven") to see an example.
   

## Contact us

Hopefully, this example project has helped you get up and running with ECS and CloudReactor.
Feel free to reach out to us at support@cloudreactor.io to setup an account, or  
if you have any questions or issues!
